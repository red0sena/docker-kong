_format_version: "3.0"

plugins:

consumers:
  - username: "llama2-client"
    custom_id: "client-001"
    tags: ["llama2-access"]
    keyauth_credentials:
      - key: "llama2-secret-key-12345"
    plugins:
      - name: user-metadata
        config:
          id: "kt-llama3-1"
          provider_name: "kt"
          tags: ["뭘봐임마"]
          tdl_matrix:
            T1: [0, 0, 0, 0, 0, 3, 0, 0, 0, 0]
            T2: [0, 0, 0, 0, 0, 3, 0, 0, 0, 0]
            T3: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
            T4: [0, 0, 0, 0, 0, 32, 0, 0, 0, 0]
            T5: [0, 0, 0, 0, 0, 3, 0, 0, 0, 0]
            T6: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
            T7: [0, 0, 0, 0, 0, 3, 0, 0, 0, 0]
            T8: [0, 0, 0, 0, 0, 3, 0, 0, 0, 0]
            T9: [0, 0, 0, 0, 0, 3, 0, 0, 0, 0]
            T10: [0, 0, 0, 0, 0, 3, 0, 0, 0, 0]
            T11: [0, 0, 0, 0, 0, 3, 0, 0, 0, 0]
            T12: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
          capabilities: [1, 1, 0, 0, 0]
          max_input_tokens: 24000
          max_output_tokens: 8000
          input_cost_per_token: 0.000000083
          output_cost_per_token: 0.00000033

  - username: "premium-client"
    custom_id: "client-002"
    tags: ["premium-access"]
    keyauth_credentials:
      - key: "premium-secret-key-67890"
    plugins:
      - name: user-metadata
        config:
          id: "premium-llama3-2"
          provider_name: "premium"
          tags: ["프리미엄", "VIP"]
          tdl_matrix:
            T1: [0, 0, 0, 0, 0, 5, 0, 0, 0, 0]
            T2: [0, 0, 0, 0, 0, 5, 0, 0, 0, 0]
            T3: [0, 0, 0, 0, 0, 5, 0, 0, 0, 0]
            T4: [0, 0, 0, 0, 0, 50, 0, 0, 0, 0]
            T5: [0, 0, 0, 0, 0, 5, 0, 0, 0, 0]
            T6: [0, 0, 0, 0, 0, 5, 0, 0, 0, 0]
            T7: [0, 0, 0, 0, 0, 5, 0, 0, 0, 0]
            T8: [0, 0, 0, 0, 0, 5, 0, 0, 0, 0]
            T9: [0, 0, 0, 0, 0, 5, 0, 0, 0, 0]
            T10: [0, 0, 0, 0, 0, 5, 0, 0, 0, 0]
            T11: [0, 0, 0, 0, 0, 5, 0, 0, 0, 0]
            T12: [0, 0, 0, 0, 0, 5, 0, 0, 0, 0]
          capabilities: [1, 1, 1, 1, 1]
          max_input_tokens: 50000
          max_output_tokens: 20000
          input_cost_per_token: 0.00000005
          output_cost_per_token: 0.0000002

services:
  - name: user-metadata-service
    url: http://httpbin.org/status/200
    connect_timeout: 5000
    read_timeout: 5000
    write_timeout: 5000
    retries: 1

  - name: llama2-api
    url: http://host.docker.internal:11434
    connect_timeout: 60000
    read_timeout: 60000
    write_timeout: 60000
    retries: 5

  - name: openai-api
    url: https://api.openai.com/v1
    connect_timeout: 60000
    read_timeout: 60000
    write_timeout: 60000
    retries: 5

routes:
  - name: user-metadata-api
    service: user-metadata-service
    paths:
      - /api/user-metadata
      - /api/tdl-matrix
    methods:
      - GET
      - POST
      - OPTIONS
    protocols:
      - http
      - https
    strip_path: false
    preserve_host: false
    path_handling: v0
    regex_priority: 300
    https_redirect_status_code: 426
    request_buffering: true
    response_buffering: true
    plugins:
      - name: key-auth
        config:
          key_names: ["apikey"]
          hide_credentials: true

  - name: llama2-api_llama2-chat
    service: llama2-api
    paths:
      - /llama2/chat
    methods:
      - POST
    protocols:
      - http
      - https
    strip_path: false
    preserve_host: false
    path_handling: v0
    regex_priority: 200
    https_redirect_status_code: 426
    request_buffering: true
    response_buffering: true
    plugins:
      - name: key-auth
        config:
          key_names: ["apikey"]
          hide_credentials: true
      - name: ai-proxy
        config:
          route_type: llm/v1/chat
          response_streaming: allow
          model_name_header: true
          llm_format: openai
          max_request_body_size: 8192
          model:
            name: llama2
            provider: llama2
            options:
              llama2_format: ollama
              upstream_url: http://host.docker.internal:11434/api/chat
              temperature: 0.7
              max_tokens: 256
          logging:
            log_payloads: false
            log_statistics: true

  - name: openai-api_chat
    service: openai-api
    paths:
      - /openai/chat
    methods:
      - POST
    protocols:
      - http
      - https
    strip_path: false
    preserve_host: false
    path_handling: v0
    regex_priority: 200
    https_redirect_status_code: 426
    request_buffering: true
    response_buffering: true
    plugins:
      - name: ai-proxy
        config:
          route_type: llm/v1/chat
          response_streaming: allow
          model_name_header: true
          llm_format: openai
          max_request_body_size: 8192
          model:
            name: gpt-3.5-turbo
            provider: openai
            options:
              temperature: 0.7
              max_tokens: 256
          auth:
            header_name: Authorization
            header_value: Bearer sk-1234
            allow_override: false
          logging:
            log_payloads: false
            log_statistics: true