_format_version: "3.0"

  - name: user-metadata-api
    service: user-metadata-service
    paths:
      - /api/user-metadata
      - /api/tdl-matrix
    methods:
      - GET
      - POST
      - OPTIONS
    protocols:
      - http
      - https
    strip_path: false
    preserve_host: false
    path_handling: v0
    regex_priority: 300
    https_redirect_status_code: 426
    request_buffering: true
    response_buffering: true
    plugins:
      - name: key-auth
        config:
          key_names: ["apikey"]
          hide_credentials: true

  - name: llama2-api_llama2-chat
    service: llama2-api
    paths:
      - /midm-base/chat
    methods:
      - POST
    protocols:
      - http
      - https
    strip_path: false
    preserve_host: false
    path_handling: v0
    regex_priority: 200
    https_redirect_status_code: 426
    request_buffering: true
    response_buffering: true
    plugins:
      - name: key-auth
        config:
          key_names: ["apikey"]
          hide_credentials: true
      - name: acl
        config:
          allow: ["midm-allowed"]
      - name: ai-proxy
        config:
          route_type: llm/v1/chat
          response_streaming: allow
          model_name_header: true
          llm_format: openai
          max_request_body_size: 8192
          model:
            name: llama2
            provider: llama2
            options:
              llama2_format: ollama
              upstream_url: http://host.docker.internal:11434/api/chat
              temperature: 0.7
              max_tokens: 256
          logging:
            log_payloads: false
            log_statistics: true

  - name: openai-api_gpt4o-chat
    service: openai-api
    paths:
      - /gpt4o/chat
    methods:
      - POST
    protocols:
      - http
      - https
    strip_path: false
    preserve_host: false
    path_handling: v0
    regex_priority: 200
    https_redirect_status_code: 426
    request_buffering: true
    response_buffering: true
    plugins:
      - name: key-auth
        config:
          key_names: ["apikey"]
          hide_credentials: true
      - name: acl
        config:
          allow: ["openai-allowed"]
      - name: ai-proxy
        config:
          route_type: llm/v1/chat
          response_streaming: allow
          model_name_header: true
          llm_format: openai
          max_request_body_size: 8192
          model:
            name: gpt-4o
            provider: openai
            options:
              temperature: 0.7
              max_tokens: 256
          auth:
            header_name: Authorization
            header_value: "{vault://myenv/openai-api-key}"
            allow_override: false
          logging:
            log_payloads: false
            log_statistics: true